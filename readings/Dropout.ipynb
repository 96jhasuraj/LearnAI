{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3209346c-8e54-4ddc-bfbe-37568092d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as tvtransforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f02adc6-b21a-459a-9199-8570ab97e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7192e178-b2f9-4771-8193-042d2ca44bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1.0\n",
    "lr_decay = 0.998\n",
    "initial_momentum = 0.5\n",
    "final_momentum = 0.99\n",
    "momentum_epochs = 15\n",
    "max_norm = 15.0 \n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "dropout_hidden = 0.5\n",
    "dropout_input = 0.2\n",
    "weight_std = 0.01\n",
    "# paper trains models for 3000 epochs . Init Lr is 10 . Init decay is 0.998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5beebd2-f84e-4e0c-8c6a-f45aa091f87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean: 0.1307, std: 0.3081\n"
     ]
    }
   ],
   "source": [
    "raw_transform = tvtransforms.ToTensor()\n",
    "raw_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=raw_transform)\n",
    "\n",
    "# Compute mean and std\n",
    "loader = DataLoader(raw_dataset, batch_size=60000, shuffle=False)\n",
    "data_iter = iter(loader)\n",
    "images, _ = next(data_iter)\n",
    "mean = images.mean().item()\n",
    "std = images.std().item()\n",
    "\n",
    "print(f\"Calculated mean: {mean:.4f}, std: {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cc9958-fba4-41b5-bac0-4b20f8ddcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_transform = tvtransforms.Compose([\n",
    "    tvtransforms.RandomRotation(10),\n",
    "    tvtransforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
    "    tvtransforms.ToTensor(), \n",
    "    tvtransforms.Normalize((mean,), (std,)),\n",
    "])\n",
    "\n",
    "mnist_test_transform = tvtransforms.Compose([\n",
    "    tvtransforms.ToTensor(), \n",
    "    tvtransforms.Normalize((mean,), (std,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1944d4-3d06-45e5-ac55-043193b2500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=mnist_train_transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=mnist_test_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ba2000-6b75-4739-8981-a4218e5ff18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrain_weights(module):\n",
    "    for name, param in module.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            l2_norms = torch.sqrt(torch.sum(param**2, dim=1, keepdim=True))\n",
    "            scale = torch.clamp(torch.sqrt(torch.tensor(max_norm)) / (l2_norms + 1e-12), max=1.0)\n",
    "            param.data *= scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57232a67-2943-45e3-8da1-74cfac94e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NN, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Dropout(p=dropout_input))\n",
    "        for i in range(len(layer_sizes) - 2):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_hidden))\n",
    "        layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1]))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=weight_std)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59b8e0-a4b9-46cc-a5de-c547961ac2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=initial_momentum)\n",
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b49aa612-c308-4b0c-815f-b4275e965e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr, momentum=initial_momentum)\n",
    "    train_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total=0\n",
    "        correct=0\n",
    "        if epoch < momentum_epochs:\n",
    "            momentum = initial_momentum + (final_momentum - initial_momentum) * epoch / momentum_epochs\n",
    "        else:\n",
    "            momentum = final_momentum\n",
    "        optimizer.param_groups[0]['momentum'] = momentum\n",
    "        \n",
    "        lr = initial_lr * (lr_decay ** epoch) * (1 - momentum)\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "        \n",
    "        model.train()\n",
    "        for data, target in tqdm(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            constrain_weights(model)\n",
    "        train_acc.append(correct/total)\n",
    "            \n",
    "    \n",
    "    return model,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83d6d4ba-ed1b-43f6-a6ec-073c03b213ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    errors = total - correct\n",
    "    error_rate = (errors / total) * 100\n",
    "    return errors, error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07fb31e2-9c16-46c2-8082-854e4d539adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_configs = [\n",
    "    [784, 800, 800, 10],\n",
    "    [784, 1200, 1200, 10],\n",
    "    [784, 1200, 1200, 1200, 10]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ae64c-8bff-444b-a021-d77cd228152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FNN with architecture: [784, 800, 800, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:29<00:00, 32.17it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:24<00:00, 37.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:25<00:00, 36.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:33<00:00, 28.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 938/938 [00:25<00:00, 36.23it/s]\n",
      " 51%|█████████████████████████████████████████                                       | 482/938 [00:12<00:12, 36.42it/s]"
     ]
    }
   ],
   "source": [
    "ffnn_results = {}\n",
    "for config in fnn_configs:\n",
    "    print(f\"Training FNN with architecture: {config}\")\n",
    "    model = NN(config).to(device)\n",
    "    model,acc  = train_model(model, train_loader, epochs)\n",
    "    errors, error_rate = evaluate_model(model, test_loader)\n",
    "    fnn_results[str(config)] = [acc,errors,error_rate]\n",
    "    print(f\"FNN {config}: {errors} errors ({error_rate:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48116a-2eff-4630-8e95-09aeac7b6bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
